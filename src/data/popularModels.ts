import type { ModelConfig } from '../types';

/**
 * Seed data for popular models — used as fallback when APIs are unreachable
 * and for quick "Popular Models" access. This is the ONLY hardcoded data.
 */
export const POPULAR_MODELS: ModelConfig[] = [
    // ─── Small (1-3B) ───
    {
        id: 'tinyllama-1.1b',
        name: 'TinyLlama 1.1B',
        organization: 'TinyLlama',
        params: 1.1,
        layers: 22,
        numAttentionHeads: 32,
        numKVHeads: 4,
        hiddenSize: 2048,
        intermediateSize: 5632,
        maxContextLength: 2048,
        isMoE: false,
        category: 'small',
        source: 'seed',
        huggingFaceId: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',
        description: 'Compact model, great for testing and constrained hardware',
    },
    {
        id: 'phi-2',
        name: 'Phi-2 2.7B',
        organization: 'Microsoft',
        params: 2.7,
        layers: 32,
        numAttentionHeads: 32,
        numKVHeads: 32,
        hiddenSize: 2560,
        intermediateSize: 10240,
        maxContextLength: 2048,
        isMoE: false,
        category: 'small',
        source: 'seed',
        huggingFaceId: 'microsoft/phi-2',
        description: 'Small but surprisingly capable for reasoning tasks',
    },
    {
        id: 'gemma-2b',
        name: 'Gemma 2B',
        organization: 'Google',
        params: 2.5,
        layers: 18,
        numAttentionHeads: 8,
        numKVHeads: 1,
        hiddenSize: 2048,
        intermediateSize: 16384,
        maxContextLength: 8192,
        isMoE: false,
        category: 'small',
        source: 'seed',
        huggingFaceId: 'google/gemma-2b',
        description: 'Google\'s lightweight open model',
    },
    {
        id: 'stablelm-2-1.6b',
        name: 'StableLM 2 1.6B',
        organization: 'Stability AI',
        params: 1.6,
        layers: 24,
        numAttentionHeads: 32,
        numKVHeads: 32,
        hiddenSize: 2048,
        intermediateSize: 5632,
        maxContextLength: 4096,
        isMoE: false,
        category: 'small',
        source: 'seed',
        huggingFaceId: 'stabilityai/stablelm-2-1_6b',
        description: 'Stability AI\'s compact language model',
    },

    // ─── Medium (7-8B) ───
    {
        id: 'llama-3.1-8b',
        name: 'Llama 3.1 8B',
        organization: 'Meta',
        params: 8.03,
        layers: 32,
        numAttentionHeads: 32,
        numKVHeads: 8,
        hiddenSize: 4096,
        intermediateSize: 14336,
        maxContextLength: 131072,
        isMoE: false,
        category: 'medium',
        source: 'seed',
        huggingFaceId: 'meta-llama/Llama-3.1-8B',
        description: 'Meta\'s flagship 8B model with 128K context',
    },
    {
        id: 'mistral-7b',
        name: 'Mistral 7B',
        organization: 'Mistral AI',
        params: 7.24,
        layers: 32,
        numAttentionHeads: 32,
        numKVHeads: 8,
        hiddenSize: 4096,
        intermediateSize: 14336,
        maxContextLength: 32768,
        isMoE: false,
        category: 'medium',
        source: 'seed',
        huggingFaceId: 'mistralai/Mistral-7B-v0.3',
        description: 'Excellent performance for its size, sliding window attention',
    },
    {
        id: 'qwen-2.5-7b',
        name: 'Qwen 2.5 7B',
        organization: 'Alibaba',
        params: 7.62,
        layers: 28,
        numAttentionHeads: 28,
        numKVHeads: 4,
        hiddenSize: 3584,
        intermediateSize: 18944,
        maxContextLength: 131072,
        isMoE: false,
        category: 'medium',
        source: 'seed',
        huggingFaceId: 'Qwen/Qwen2.5-7B',
        description: 'Alibaba\'s latest multilingual model',
    },
    {
        id: 'deepseek-r1-7b',
        name: 'DeepSeek-R1 7B',
        organization: 'DeepSeek',
        params: 7.62,
        layers: 28,
        numAttentionHeads: 28,
        numKVHeads: 4,
        hiddenSize: 3584,
        intermediateSize: 18944,
        maxContextLength: 131072,
        isMoE: false,
        category: 'medium',
        source: 'seed',
        huggingFaceId: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
        description: 'DeepSeek\'s R1 distilled reasoning model',
    },
    {
        id: 'gemma-7b',
        name: 'Gemma 7B',
        organization: 'Google',
        params: 8.54,
        layers: 28,
        numAttentionHeads: 16,
        numKVHeads: 16,
        hiddenSize: 3072,
        intermediateSize: 24576,
        maxContextLength: 8192,
        isMoE: false,
        category: 'medium',
        source: 'seed',
        huggingFaceId: 'google/gemma-7b',
        description: 'Google\'s 7B open model',
    },

    // ─── Large (13-14B) ───
    {
        id: 'llama-2-13b',
        name: 'Llama 2 13B',
        organization: 'Meta',
        params: 13.02,
        layers: 40,
        numAttentionHeads: 40,
        numKVHeads: 40,
        hiddenSize: 5120,
        intermediateSize: 13824,
        maxContextLength: 4096,
        isMoE: false,
        category: 'large',
        source: 'seed',
        huggingFaceId: 'meta-llama/Llama-2-13b-hf',
        description: 'Meta\'s 13B model, great balance of size and capability',
    },
    {
        id: 'qwen-2.5-14b',
        name: 'Qwen 2.5 14B',
        organization: 'Alibaba',
        params: 14.77,
        layers: 48,
        numAttentionHeads: 40,
        numKVHeads: 8,
        hiddenSize: 5120,
        intermediateSize: 13824,
        maxContextLength: 131072,
        isMoE: false,
        category: 'large',
        source: 'seed',
        huggingFaceId: 'Qwen/Qwen2.5-14B',
        description: 'Alibaba\'s 14B model with 128K context',
    },

    // ─── XL (27-34B) ───
    {
        id: 'gemma-2-27b',
        name: 'Gemma 2 27B',
        organization: 'Google',
        params: 27.23,
        layers: 46,
        numAttentionHeads: 32,
        numKVHeads: 16,
        hiddenSize: 4608,
        intermediateSize: 36864,
        maxContextLength: 8192,
        isMoE: false,
        category: 'xl',
        source: 'seed',
        huggingFaceId: 'google/gemma-2-27b',
        description: 'Google\'s largest Gemma model',
    },
    {
        id: 'codellama-34b',
        name: 'CodeLlama 34B',
        organization: 'Meta',
        params: 33.74,
        layers: 48,
        numAttentionHeads: 64,
        numKVHeads: 8,
        hiddenSize: 8192,
        intermediateSize: 22016,
        maxContextLength: 16384,
        isMoE: false,
        category: 'xl',
        source: 'seed',
        huggingFaceId: 'meta-llama/CodeLlama-34b-hf',
        description: 'Specialized for code generation',
    },
    {
        id: 'deepseek-r1-32b',
        name: 'DeepSeek-R1 32B',
        organization: 'DeepSeek',
        params: 32.76,
        layers: 64,
        numAttentionHeads: 64,
        numKVHeads: 8,
        hiddenSize: 5120,
        intermediateSize: 12288,
        maxContextLength: 131072,
        isMoE: false,
        category: 'xl',
        source: 'seed',
        huggingFaceId: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',
        description: 'DeepSeek\'s 32B distilled reasoning model',
    },

    // ─── XXL (70B+) ───
    {
        id: 'llama-3.1-70b',
        name: 'Llama 3.1 70B',
        organization: 'Meta',
        params: 70.55,
        layers: 80,
        numAttentionHeads: 64,
        numKVHeads: 8,
        hiddenSize: 8192,
        intermediateSize: 28672,
        maxContextLength: 131072,
        isMoE: false,
        category: 'xxl',
        source: 'seed',
        huggingFaceId: 'meta-llama/Llama-3.1-70B',
        description: 'Meta\'s 70B flagship — needs serious hardware',
    },
    {
        id: 'qwen-2.5-72b',
        name: 'Qwen 2.5 72B',
        organization: 'Alibaba',
        params: 72.71,
        layers: 80,
        numAttentionHeads: 64,
        numKVHeads: 8,
        hiddenSize: 8192,
        intermediateSize: 29568,
        maxContextLength: 131072,
        isMoE: false,
        category: 'xxl',
        source: 'seed',
        huggingFaceId: 'Qwen/Qwen2.5-72B',
        description: 'Alibaba\'s 72B frontier model',
    },

    // ─── MoE Models ───
    {
        id: 'mixtral-8x7b',
        name: 'Mixtral 8x7B',
        organization: 'Mistral AI',
        params: 46.7,
        layers: 32,
        numAttentionHeads: 32,
        numKVHeads: 8,
        hiddenSize: 4096,
        intermediateSize: 14336,
        maxContextLength: 32768,
        isMoE: true,
        activeParams: 12.9,
        numExperts: 8,
        numActiveExperts: 2,
        category: 'moe',
        source: 'seed',
        huggingFaceId: 'mistralai/Mixtral-8x7B-v0.1',
        description: 'Mixture-of-Experts: 8 experts, 2 active per token',
    },
    {
        id: 'mixtral-8x22b',
        name: 'Mixtral 8x22B',
        organization: 'Mistral AI',
        params: 140.6,
        layers: 56,
        numAttentionHeads: 48,
        numKVHeads: 8,
        hiddenSize: 6144,
        intermediateSize: 16384,
        maxContextLength: 65536,
        isMoE: true,
        activeParams: 39.1,
        numExperts: 8,
        numActiveExperts: 2,
        category: 'moe',
        source: 'seed',
        huggingFaceId: 'mistralai/Mixtral-8x22B-v0.1',
        description: 'Large MoE: 8x22B experts, 2 active per token',
    },
    {
        id: 'deepseek-v3',
        name: 'DeepSeek-V3',
        organization: 'DeepSeek',
        params: 671,
        layers: 61,
        numAttentionHeads: 128,
        numKVHeads: 128,
        hiddenSize: 7168,
        intermediateSize: 18432,
        maxContextLength: 131072,
        isMoE: true,
        activeParams: 36.7,
        numExperts: 256,
        numActiveExperts: 8,
        category: 'moe',
        source: 'seed',
        huggingFaceId: 'deepseek-ai/DeepSeek-V3',
        description: 'DeepSeek\'s 671B MoE — 256 experts, 8 active per token',
    },
];

export const MODEL_CATEGORIES: Record<string, string> = {
    small: 'Small (1-3B)',
    medium: 'Medium (7-8B)',
    large: 'Large (13-14B)',
    xl: 'XL (27-34B)',
    xxl: 'XXL (70B+)',
    moe: 'MoE',
};
